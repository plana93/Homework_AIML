{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_pipeline_MS.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOPRpKOdSDiVSsc1wW6aB6s"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"cPAQpsvWynjD","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dfiRjXH3yQ6A","colab_type":"code","colab":{}},"source":["%cd drive/My\\ Drive/\n","!git clone https://[username]:[password]@github.com/Erosinho13/FPAR-Project-MLDL.git\n","%cd FPAR-Project-MLDL/"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v2lsdTrTBZrA","colab_type":"text"},"source":["# SETUP\n","The following blocks are to be executed first of anything else, to setup import, classes, functions and \n","constants that are needed for all stages"]},{"cell_type":"code","metadata":{"id":"9G569ps3BdBw","colab_type":"code","colab":{}},"source":["import os\n","import logging\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torch.backends import cudnn\n","\n","import torchvision\n","from torchvision import transforms\n","from torchvision.models import resnet34\n","\n","from PIL import Image\n","from tqdm import tqdm\n","\n","from logs import Logger, generate_model_checkpoint_name, generate_log_filenames\n","\n","from gtea_dataset import GTEA61, GTEA61_flow, GTEA61_2Stream\n","from AttentionModelMS import attention_model_ms\n","from flow_resnet import flow_resnet34\n","from twoStreamModel import twoStreamAttentionModel\n","from spatial_transforms import (Compose, ToTensor, CenterCrop, Scale, Normalize, MultiScaleCornerCrop,\n","                                RandomHorizontalFlip, DownSampling, To1Dimension)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7IX0KVsVBjwh","colab_type":"code","colab":{}},"source":["DEVICE = 'cuda' # 'cuda' or 'cpu'\n","NUM_CLASSES = 61 # 101 + 1: There is am extra Background class that should be removed \n","\n","BATCH_SIZE = 32     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n","                     # the batch size, learning rate should change by the same factor to have comparable results\n","\n","MMAP_LENGTH = 49\n","DATA_DIR = '../GTEA61'\n","model_folder = '../saved_models'\n","\n","RUN = '_run03'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gjo-WeDUCKtm","colab_type":"code","colab":{}},"source":["# Data loader\n","normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","default_transforms = [\n","    Scale(256),\n","    RandomHorizontalFlip(),\n","    MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n","    ToTensor()\n","]\n","\n","spatial_transform = Compose(default_transforms + [normalize])\n","spatial_transform_mmaps = Compose(default_transforms + [DownSampling(), To1Dimension()])\n","spatial_transform_val = Compose([Scale(256), CenterCrop(224), ToTensor(), normalize])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t9wsXdedDE90","colab_type":"text"},"source":["# Stage 1 specific-setup"]},{"cell_type":"code","metadata":{"id":"dO0j_wjwDARB","colab_type":"code","colab":{}},"source":["STAGE = 1\n","\n","LR = 0.001                            # The initial Learning Rate\n","MOMENTUM = 0.9                        # Hyperparameter for SGD, keep this at 0.9 when using SGD\n","WEIGHT_DECAY = 4e-5                   # Regularization, you can keep this at the default\n","\n","NUM_EPOCHS = 200                      # Total number of training epochs (iterations over dataset)\n","STEP_SIZE = [25, 75, 150]             # How many epochs before decreasing learning rate (if using a step-down policy)\n","GAMMA = 0.1                           # Multiplicative factor for learning rate step-down\n","MEM_SIZE = 512\n","SEQ_LEN = 16\n","\n","# this dictionary is needed for the logger class\n","parameters = {\n","    'DEVICE': DEVICE,\n","    'NUM_CLASSES': NUM_CLASSES,\n","    'BATCH_SIZE': BATCH_SIZE,\n","    'LR': LR,\n","    'MOMENTUM': MOMENTUM,\n","    'WEIGHT_DECAY': WEIGHT_DECAY,\n","    'NUM_EPOCHS': NUM_EPOCHS,\n","    'STEP_SIZE': STEP_SIZE,\n","    'GAMMA': GAMMA,\n","    'MEM_SIZE': MEM_SIZE,\n","    'SEQ_LEN': SEQ_LEN\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wfttpj_7DikR","colab_type":"code","outputId":"a5c516f4-9982-413b-b4e3-039e9e2e5cfd","executionInfo":{"status":"ok","timestamp":1591306687647,"user_tz":-120,"elapsed":1534,"user":{"displayName":"Eros Fanì","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhapRbf5lUkHjaoCtjruM95BBemedtJFkNrcJgVKA=s64","userId":"05010694830681132177"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Prepare Pytorch train/test Datasets\n","train_dataset = GTEA61(DATA_DIR, split = 'train', transform = spatial_transform, seq_len = SEQ_LEN,\n","                       mmaps = True, mmaps_transform = spatial_transform_mmaps)\n","test_dataset = GTEA61(DATA_DIR, split = 'test', transform = spatial_transform_val, seq_len = SEQ_LEN)\n","\n","# Check dataset sizes\n","print('Train Dataset: {}'.format(len(train_dataset)))\n","print('Test Dataset: {}'.format(len(test_dataset)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train Dataset: 341\n","Test Dataset: 116\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x4PJpn_zD97F","colab_type":"code","colab":{}},"source":["# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n","train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4, drop_last = True)\n","val_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = False, num_workers = 4)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6oToJxBGEH1U","colab_type":"text"},"source":["# Prepare stage 1"]},{"cell_type":"code","metadata":{"id":"ud6mw-NYED3C","colab_type":"code","colab":{}},"source":["validate = True\n","\n","model = attention_model_ms(num_classes = NUM_CLASSES, mem_size = MEM_SIZE)\n","model.train(False)\n","\n","for params in model.parameters():\n","    params.requires_grad = False\n","\n","for params in model.lstm_cell.parameters():\n","    params.requires_grad = True\n","\n","for params in model.classifier.parameters():\n","    params.requires_grad = True\n","\n","model.lstm_cell.train(True)\n","model.classifier.train(True)\n","model.to(DEVICE)\n","\n","trainable_params = [p for p in model.parameters() if p.requires_grad]\n","\n","loss_fn = nn.CrossEntropyLoss()\n","\n","optimizer_fn = optim.Adam(trainable_params, lr = LR, weight_decay = WEIGHT_DECAY, eps = 1e-4)\n","\n","optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones = STEP_SIZE, gamma = GAMMA)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"laVQMCiNFVb2","colab_type":"text"},"source":["# Stage 1"]},{"cell_type":"code","metadata":{"id":"nWwgaMAMFQXs","colab_type":"code","colab":{}},"source":["train_iter = 0\n","val_iter = 0\n","min_accuracy = 0\n","\n","trainSamples = len(train_dataset) - (len(train_dataset) % BATCH_SIZE)\n","val_samples = len(test_dataset) \n","iterPerEpoch = len(train_loader)\n","val_steps = len(val_loader)\n","\n","cudnn.benchmark\n","\n","train_log, val_log = generate_log_filenames(STAGE, SEQ_LEN, ms_block = True, \\\n","                                            optional = RUN)\n","model_checkpoint = generate_model_checkpoint_name(STAGE, SEQ_LEN, ms_block = True, \\\n","                                                  optional = RUN)\n","\n","train_log_file = os.path.join(model_folder, train_log)\n","val_log_file = os.path.join(model_folder, val_log)\n","\n","train_logger = Logger(**parameters)\n","val_logger = Logger(**parameters)\n","\n","for epoch in range(NUM_EPOCHS):\n","\n","    epoch_loss = 0\n","    numCorrTrain = 0\n","    \n","    model.lstm_cell.train(True)\n","    model.classifier.train(True)\n","        \n","    for i, (inputs, _, targets) in enumerate(train_loader):\n","\n","        train_iter += 1\n","\n","        optimizer_fn.zero_grad()\n","\n","        inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n","        labelVariable = targets.to(DEVICE)\n","        output_label, _ = model(inputVariable)\n","        \n","        loss = loss_fn(output_label, labelVariable)\n","        loss.backward()\n","        optimizer_fn.step()\n","            \n","        _, predicted = torch.max(output_label.data, 1)\n","        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n","\n","        step_loss = loss.data.item()\n","        epoch_loss += step_loss\n","\n","        train_logger.add_step_data(train_iter, numCorrTrain, step_loss)\n","    \n","    avg_loss = epoch_loss/iterPerEpoch\n","\n","    trainAccuracy = (numCorrTrain / trainSamples) * 100\n","    train_logger.add_epoch_data(epoch+1, trainAccuracy, avg_loss)\n","    print('Train: Epoch = {} | Loss = {:.3f} | Accuracy = {:.3f}'.format(epoch+1, avg_loss, trainAccuracy))\n","\n","    if validate:\n","\n","        if (epoch+1) % 1 == 0:\n","\n","            model.train(False)\n","\n","            val_loss_epoch = 0\n","            numCorr = 0\n","\n","            for j, (inputs, targets) in enumerate(val_loader):\n","\n","                val_iter += 1\n","\n","                inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n","                labelVariable = targets.to(DEVICE)\n","                \n","                output_label, _ = model(inputVariable)\n","\n","                val_loss = loss_fn(output_label, labelVariable)\n","                val_loss_step = val_loss.data.item()\n","                val_loss_epoch += val_loss_step\n","                \n","                _, predicted = torch.max(output_label.data, 1)\n","                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n","\n","                val_logger.add_step_data(val_iter, numCorr, val_loss_step)\n","                \n","            val_accuracy = (numCorr / val_samples) * 100\n","            avg_val_loss = val_loss_epoch / val_steps\n","            val_logger.add_epoch_data(epoch+1, val_accuracy, avg_val_loss)\n","\n","            print('Val: Epoch = {} | Loss {:.3f} | Accuracy = {:.3f}'.format(epoch + 1, avg_val_loss, val_accuracy))\n","\n","            if val_accuracy > min_accuracy:\n","                print(\"[||| NEW BEST on val||||]\")\n","                save_path_model = os.path.join(model_folder, model_checkpoint)\n","                torch.save(model.state_dict(), save_path_model)\n","                min_accuracy = val_accuracy\n","        \n","    train_logger.save(train_log_file)\n","    val_logger.save(val_log_file)\n","    optim_scheduler.step()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mjwHu7nlwPOO","colab_type":"text"},"source":["# Stage 2 specific-setup"]},{"cell_type":"code","metadata":{"id":"hWvGKBdUwQUf","colab_type":"code","colab":{}},"source":["STAGE = 2\n","\n","LR = 0.0001            # The initial Learning Rate\n","MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n","WEIGHT_DECAY = 4e-5  # Regularization, you can keep this at the default\n","\n","NUM_EPOCHS = 150      # Total number of training epochs (iterations over dataset)\n","STEP_SIZE = [25, 75] # How many epochs before decreasing learning rate (if using a step-down policy)\n","GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n","MEM_SIZE = 512\n","SEQ_LEN = 16\n","\n","parameters = {\n","    'DEVICE': DEVICE,\n","    'NUM_CLASSES': NUM_CLASSES,\n","    'BATCH_SIZE': BATCH_SIZE,\n","    'LR': LR,\n","    'MOMENTUM': MOMENTUM,\n","    'WEIGHT_DECAY': WEIGHT_DECAY,\n","    'NUM_EPOCHS': NUM_EPOCHS,\n","    'STEP_SIZE': STEP_SIZE,\n","    'GAMMA': GAMMA,\n","    'MEM_SIZE': MEM_SIZE,\n","    'SEQ_LEN': SEQ_LEN\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y6LvKvL7_0ND","colab_type":"code","outputId":"ea4fd117-e7d4-4661-e4b4-fcffeb096705","executionInfo":{"status":"ok","timestamp":1591525635224,"user_tz":-120,"elapsed":148382,"user":{"displayName":"Eros Fanì","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhapRbf5lUkHjaoCtjruM95BBemedtJFkNrcJgVKA=s64","userId":"05010694830681132177"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Prepare Pytorch train/test Datasets\n","train_dataset = GTEA61(DATA_DIR, split='train', transform=spatial_transform,\n","                       seq_len=SEQ_LEN, mmaps = True, mmaps_transform = spatial_transform_mmaps)\n","test_dataset = GTEA61(DATA_DIR, split='test', transform=spatial_transform_val,\n","                      seq_len=SEQ_LEN)\n","\n","# Check dataset sizes\n","print('Train Dataset: {}'.format(len(train_dataset)))\n","print('Test Dataset: {}'.format(len(test_dataset)))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Train Dataset: 341\n","Test Dataset: 116\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gmnYcBVT_zHm","colab_type":"code","colab":{}},"source":["# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cmCrfp8AABdl","colab_type":"text"},"source":["# Prepare stage 2"]},{"cell_type":"code","metadata":{"id":"sXMbHelUwWv2","colab_type":"code","colab":{}},"source":["best_old_stage = generate_model_checkpoint_name(stage=1, n_frames=SEQ_LEN, ms_block = True)\n","stage1_dict = os.path.join(model_folder, best_old_stage)\n","validate = True\n","\n","model = attention_model_ms(num_classes=NUM_CLASSES, mem_size=MEM_SIZE)\n","model.load_state_dict(torch.load(stage1_dict))\n","\n","model.train(False)\n","\n","for params in model.parameters():\n","    params.requires_grad = False\n","\n","layers_to_train = [\n","    model.resNet.layer4[0].conv1,\n","    model.resNet.layer4[0].conv2,\n","    model.resNet.layer4[1].conv1,\n","    model.resNet.layer4[1].conv2,\n","    model.resNet.layer4[2].conv1,\n","    model.resNet.layer4[2].conv2,\n","    model.resNet.fc,\n","    model.lstm_cell,\n","    model.classifier,\n","    model.msBlock\n","]\n","\n","for layer in layers_to_train:\n","    for params in layer.parameters():\n","        params.requires_grad = True\n","\n","for layer in layers_to_train:\n","    layer.train(True)\n","\n","model.to(DEVICE)\n","\n","loss_fn = nn.CrossEntropyLoss()\n","loss_fn_sum = nn.CrossEntropyLoss(reduction = 'sum')\n","\n","trainable_params = [p for p in model.parameters() if p.requires_grad]\n","optimizer_fn = torch.optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n","\n","optim_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DmAez0fl_uCY","colab_type":"text"},"source":["# Stage 2"]},{"cell_type":"code","metadata":{"id":"oe-sovUP_u7M","colab_type":"code","outputId":"3f8db7d6-ac95-4f02-ed9f-b4317b2cead7","executionInfo":{"status":"ok","timestamp":1591324728032,"user_tz":-120,"elapsed":8263321,"user":{"displayName":"Eros Fanì","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhapRbf5lUkHjaoCtjruM95BBemedtJFkNrcJgVKA=s64","userId":"05010694830681132177"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["train_iter = 0\n","val_iter = 0\n","min_accuracy = 0\n","\n","trainSamples = len(train_dataset) - (len(train_dataset) % BATCH_SIZE)\n","val_samples = len(test_dataset)\n","\n","iterPerEpoch = len(train_loader)\n","val_steps = len(val_loader)\n","\n","cudnn.benchmark\n","torch.autograd.set_detect_anomaly(True)\n","train_log, val_log = generate_log_filenames(STAGE, SEQ_LEN, ms_block = True)\n","model_checkpoint = generate_model_checkpoint_name(STAGE, SEQ_LEN, ms_block = True)\n","\n","train_log_file = os.path.join(model_folder, train_log)\n","val_log_file = os.path.join(model_folder, val_log)\n","train_logger_2 = Logger(**parameters)\n","val_logger_2 = Logger(**parameters)\n","\n","for epoch in range(NUM_EPOCHS):\n","\n","    label_epoch_loss = 0\n","    mmaps_epoch_loss = 0\n","    numCorrTrain = 0\n","    \n","    for layer in layers_to_train:\n","        layer.train(True)\n","    \n","    for i, (inputs, mmaps, targets) in enumerate(train_loader):\n","\n","        mmaps = mmaps.permute(1, 0, 2)\n","\n","        train_iter += 1\n","\n","        optimizer_fn.zero_grad()\n","\n","        inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n","        labelVariable = targets.to(DEVICE)\n","\n","        output_label, _, output_label_mmaps = model(inputVariable, mmaps = True)\n","\n","        output_label_mmaps = torch.flatten(output_label_mmaps).unsqueeze_(-1) + torch.zeros(2).to(DEVICE)\n","        output_label_mmaps[:, 0] = 1 - output_label_mmaps[:, 1]\n","        output_label_mmaps = output_label_mmaps.to(DEVICE)\n","        \n","        mmaps = torch.flatten(mmaps).long().to(DEVICE)\n","\n","        mmaps_loss = loss_fn_sum(output_label_mmaps, mmaps)/(BATCH_SIZE * SEQ_LEN)\n","        label_loss = loss_fn(output_label, labelVariable)\n","        loss = mmaps_loss + label_loss\n","\n","        loss.backward()\n","\n","        optimizer_fn.step()\n","        \n","        _, predicted = torch.max(output_label.data, 1)\n","        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n","        \n","        label_step_loss = label_loss.data.item()\n","        label_epoch_loss += label_step_loss\n","        \n","        mmaps_step_loss = mmaps_loss.data.item()\n","        mmaps_epoch_loss += mmaps_step_loss\n","\n","        # train_logger_2.add_step_data(train_iter, numCorrTrain, step_loss)\n","        \n","    label_avg_loss = label_epoch_loss/iterPerEpoch\n","    mmaps_avg_loss = mmaps_epoch_loss/iterPerEpoch\n","    trainAccuracy = (numCorrTrain / trainSamples) * 100\n","    train_logger_2.add_epoch_data(epoch+1, trainAccuracy, label_avg_loss, mmaps_avg_loss)\n","    \n","    print('Train: Epoch = {} | label_avg_loss = {:.3f} | mmaps_avg_loss = {:.3f} | Accuracy = {:.3f}'.format(\n","        epoch+1, label_avg_loss, mmaps_avg_loss, trainAccuracy))\n","\n","    if validate is not None:\n","\n","        if (epoch+1) % 1 == 0:\n","\n","            model.train(False)\n","            val_loss_epoch = 0\n","            numCorr = 0\n","\n","            for j, (inputs, targets) in enumerate(val_loader):\n","\n","                val_iter += 1\n","\n","                inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n","                labelVariable = targets.to(DEVICE)\n","                \n","                output_label, _ = model(inputVariable)\n","\n","                val_loss = loss_fn(output_label, labelVariable)\n","                val_loss_step = val_loss.data.item()\n","                val_loss_epoch += val_loss_step\n","\n","                _, predicted = torch.max(output_label.data, 1)\n","\n","                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n","                val_logger_2.add_step_data(val_iter, numCorr, val_loss_step)\n","\n","            val_accuracy = (numCorr / val_samples) * 100\n","            avg_val_loss = val_loss_epoch / val_steps\n","\n","            print('Val: Epoch = {} | Loss {:.3f} | Accuracy = {:.3f}'.format(epoch + 1, avg_val_loss, val_accuracy))\n","            \n","            val_logger_2.add_epoch_data(epoch+1, val_accuracy, avg_val_loss)\n","            \n","            if val_accuracy > min_accuracy:\n","                print(\"[||| NEW BEST on val |||]\")\n","                save_path_model = os.path.join(model_folder, model_checkpoint)\n","                torch.save(model.state_dict(), save_path_model)\n","                min_accuracy = val_accuracy\n","            \n","    optim_scheduler.step()\n","\n","    train_logger_2.save(train_log_file)\n","    val_logger_2.save(val_log_file)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train: Epoch = 1 | label_avg_loss = 2.179 | mmaps_avg_loss = 18.124 | Accuracy = 40.000\n","Val: Epoch = 1 | Loss 1.994 | Accuracy = 46.552\n","[||| NEW BEST on val |||]\n","Train: Epoch = 2 | label_avg_loss = 2.011 | mmaps_avg_loss = 18.241 | Accuracy = 45.000\n","Val: Epoch = 2 | Loss 1.945 | Accuracy = 44.828\n","Train: Epoch = 3 | label_avg_loss = 1.969 | mmaps_avg_loss = 18.172 | Accuracy = 44.688\n","Val: Epoch = 3 | Loss 1.995 | Accuracy = 49.138\n","[||| NEW BEST on val |||]\n","Train: Epoch = 4 | label_avg_loss = 1.830 | mmaps_avg_loss = 18.227 | Accuracy = 46.875\n","Val: Epoch = 4 | Loss 1.765 | Accuracy = 46.552\n"],"name":"stdout"}]}]}